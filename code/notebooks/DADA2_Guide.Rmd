---
title: '16S Analysis '
author: "Sarah Lucas"
output:
  html_document:
    df_print: paged
---

# Purpose

This analysis will produce a table of amplicon sequence variants and taxonomy.

# Materials
* Publication - ["Tacrolimus Pharmacokinetics is Associated with Gut Microbiota Diversity in Kidney Transplant Patients: Results from a Pilot Cross-Sectional Study"](https://ascpt.onlinelibrary.wiley.com/doi/10.1002/cpt.3077)

* SRA BioProject: [PRJNA929904](https://www.ncbi.nlm.nih.gov/Traces/study/?acc=PRJNA929904&o=acc_s%3Aa)

* [DADA2 Tutorial](https://benjjneb.github.io/dada2/tutorial.html)

# Procedure

## Setup

### Load DADA2 library

```{r echo=FALSE}
library(dada2); packageVersion("dada2")
library(here)
library(tidyverse)
```

### Point to data

We're going to use the here() function to assign the location of our data to a variable called Path
```{r datapath}
path <- here("data/raw_fastq")
list.files(path)
```

Now we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.

```{r fnFsRs}
# Forward and reverse fastq files
fnFs <- sort(list.files(path, pattern="_1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_2.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

## Quality Assessment

### Plot Quality

Look at overall quality of the data

```{r plotquality}
plotQualityProfile(fnFs[1:10])
plotQualityProfile(fnRs[1:10])
```
As expected, Read 1 files have overall very high quality. Read 2 files have lower quality after ~150 bp


### Filter and Trim

Make a path for the processed data to go (we don't want to put it in the raw_fastq folder)
```{r}
processed_path <- here("data/processed")

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(processed_path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(processed_path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Filter and trim forward and reverse reads using default parameters
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(200,150),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=24)
head(out)
```
## Learn the Error Rates
```{r errorrates}
errF <- learnErrors(filtFs, multithread=24)
errR <- learnErrors(filtRs, multithread=24)
plotErrors(errF, nominalQ=TRUE)
```

The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence.


## Sample inference
Apply the error model to correct the filtered dataset
```{r inferErrors}
dadaFs <- dada(filtFs, err=errF, multithread=24)
dadaRs <- dada(filtRs, err=errR, multithread=24)
```

Inspect the corrected forward reads
```{r}
dadaFs[[1]]
```
The DADA2 algorithm inferred 260 true sequence variants from the 9916 unique sequences in the first sample. There is much more to the dada-class return object than this (see help("dada-class") for some info), including multiple diagnostics about the quality of each denoised sequence variant, but that is beyond the scope of an introductory tutorial


## Merge paired reads

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```
The mergers object is a list of data.frames from each sample. Each data.frame contains the merged $sequence, its $abundance, and the indices of the $forward and $reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.

## Construct Sequence Table

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

```{r}
# Inspect distribution of sequence lengths
seqlength <- table(nchar(getSequences(seqtab)))
ggplot(as.data.frame(seqlength), aes(x=Var1, y = Freq)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5)) +
  labs(x = "sequence length", y = "frequency")
```
The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. This table contains 475061 ASVs, and the lengths of our merged sequences .

### Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=24, verbose=TRUE)
dim(seqtab.nochim)
```

```{r}
sum(seqtab.nochim)/sum(seqtab)
```

The frequency of chimeric sequences varies substantially from dataset to dataset, and depends on on factors including experimental procedures and sample complexity. Here chimeras make up about 21% of the merged sequence variants, but when we account for the abundances of those variants we see they account for only about 4% of the merged sequence reads.


## Track Reads

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

Looks good! We kept the majority of our raw reads, and there is no over-large drop associated with any single step.

> Considerations for your own data: This is a great place to do a last sanity check. Outside of filtering, there should no step in which a majority of reads are lost. E.g.
  * If a majority of reads failed to merge, you may need to revisit the truncLen parameter used in the filtering step and make sure that the truncated reads span your amplicon. 
  * If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification.
 

## Assign Taxonomy

Databases were acquired from the DADA2 formatted files [here](https://zenodo.org/records/14169026)

Genus level table
```{r}
taxa <- assignTaxonomy(seqtab.nochim, 
                       here("resources/databases/silva_nr99_v138.2_toGenus_trainset.fa.gz"), 
                       multithread=24)
```

Add species
```{r}
taxa <- addSpecies(taxa, "resources/databases/silva_v138.2_assignSpecies.fa.gz")
```

Inspect Taxonomy results
```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

## Export data tables

```{r}
write_csv(seqtab.nochim, "data/processed/dada2/seqtab.csv")
write_csv(taxa, "data/processed/dada2/taxa.csv")
```


